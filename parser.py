# -*- coding: utf-8 -*-
"""parser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vL-Ig4vu5GwLfbiNVnEIBN6isY3rZffm
"""

import requests
from bs4 import BeautifulSoup
from time import sleep
import csv
from urllib.parse import urljoin
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('parser.log', encoding='utf-8'),  # üíæ –í —Ñ–∞–π–ª
        logging.StreamHandler()  # üì∫ –í –∫–æ–Ω—Å–æ–ª—å
    ]
)

def get_url(parser_dict:dict, url:str, page_num=1):
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/117.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "ru,en;q=0.9",
    }
    
    logging.info(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: {url}")
    response = requests.get(url, headers)

    soup = BeautifulSoup(response.text,'lxml')
    general_data = soup.find_all('a',class_='card-full-news _parts-news')

    for data in general_data:
        sleep(1)
        name = data.find('h3',class_='card-full-news__title').text
        parser_dict[name] = urljoin("https://lenta.ru", data.get("href"))

    logging.info(f"–ù–∞–π–¥–µ–Ω–æ {len(general_data)} –Ω–æ–≤–æ—Å—Ç–µ–π. –í—Å–µ–≥–æ: {len(parser_dict)}")

    add_data = soup.find('a',class_='loadmore js-loadmore')

    if add_data:
        add_url = urljoin('https://lenta.ru', add_data.get('href'))
        return get_url(parser_dict, add_url, page_num + 1)
    else:
        logging.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ! –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ {len(parser_dict)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        return parser_dict

logging.info("–ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫...")
url = 'https://lenta.ru/parts/news/'
parser_dict = get_url(dict(), url)

def get_text(parser_dict: dict):
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/117.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "ru,en;q=0.9",
    }

    bad_url = 0
    total = len(parser_dict)
    
    logging.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {total} —Å—Ç—Ä–∞–Ω–∏—Ü...")
    
    for i, (title, url) in enumerate(parser_dict.items(), 1):
        progress = (i / total) * 100
        logging.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{total} ({progress:.1f}%) - {title[:30]}...")
        
        response = requests.get(url, headers)
        soup = BeautifulSoup(response.text, 'lxml')

        general_data = soup.find('div', class_='topic-page__container')
        try:
            all_texts = general_data.find_all('p', class_='topic-body__content-text')
        except Exception as e:
            bad_url += 1
            logging.error(f"–û—à–∏–±–∫–∞: {url}")
            continue

        text = ' '.join(p.get_text() for p in all_texts)
        with open("parser.csv", "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=["title", "text"])
            if f.tell() == 0:
                writer.writeheader()
            writer.writerow({"title": title, "text": text})
    
    logging.info(f"–ì–æ—Ç–æ–≤–æ! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total}, –û—à–∏–±–æ–∫: {bad_url}")

get_text(parser_dict)