import requests
from bs4 import BeautifulSoup
import logging
from urllib.parse import urljoin, urlparse
from datetime import datetime
import time
import sys
import re

# Настройка логирования
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('moslenta_parser.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class MoslentaNewsParser:
    def __init__(self, headers=None, timeout=15):
        self.session = requests.Session()
        self.session.headers.update(headers or {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
        })
        self.timeout = timeout

    def fetch_page(self, url):
        """Загружает страницу и обрабатывает сетевые ошибки."""
        try:
            logger.info(f"Загружаем страницу: {url}")
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            
            # Проверяем, что получили HTML, а не капчу или ошибку
            if 'text/html' not in response.headers.get('content-type', ''):
                logger.warning(f"Получен не HTML контент: {response.headers.get('content-type')}")
                return None
                
            # Устанавливаем правильную кодировку
            if response.encoding.lower() != 'utf-8':
                response.encoding = 'utf-8'
                
            return response.text

        except requests.exceptions.ConnectionError as e:
            logger.error(f"Ошибка подключения к {url}: {e}")
        except requests.exceptions.Timeout as e:
            logger.error(f"Таймаут при запросе к {url}: {e}")
        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code
            logger.error(f"HTTP ошибка {status_code} для {url}")
            
            # Особые случаи HTTP ошибок
            if status_code == 403:
                logger.error("Доступ запрещен. Возможно, блокировка по IP или требуется CAPTCHA")
            elif status_code == 404:
                logger.error("Страница не найдена")
            elif status_code == 429:
                logger.error("Слишком много запросов. Необходимо снизить частоту")
            elif status_code >= 500:
                logger.error("Проблемы на стороне сервера")
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Общая ошибка запроса для {url}: {e}")
        except Exception as e:
            logger.error(f"Неожиданная ошибка при загрузке {url}: {e}")
            
        return None

    def parse_news_page(self, url):
        """Парсит конкретную страницу новости на moslenta.ru"""
        logger.info(f"Начинаем парсинг новости: {url}")
        
        html = self.fetch_page(url)
        if not html:
            return None

        # Сохраняем HTML для отладки в случае проблем
        debug_filename = f"debug_moslenta_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            news_data = {
                'url': url,
                'title': None,
                'date': None,
                'author': None,
                'lead': None,  # лид-абзац
                'content': None,
                'tags': [],
                'image_url': None,
                'parsing_time': datetime.now().isoformat(),
                'success': False
            }

            # 1. Парсинг заголовка (основной селектор + резервные)
            title = self._safe_extract(
                soup,
                [
                    'h1.news-head__title',
                    'h1.news-head-title',
                    'h1',
                    '.news-head h1',
                    '.article-title'
                ],
                'заголовок'
            )
            news_data['title'] = title

            # 2. Парсинг даты публикации
            date_str = self._safe_extract(
                soup,
                [
                    '.news-head__date',
                    '.article-date',
                    '.publication-date',
                    'time',
                    '.news-head time'
                ],
                'дата'
            )
            news_data['date'] = self._parse_moslenta_date(date_str)

            # 3. Парсинг автора
            author = self._safe_extract(
                soup,
                [
                    '.news-head__author',
                    '.article-author',
                    '.author-name'
                ],
                'автор'
            )
            news_data['author'] = author

            # 4. Парсинг лида (первого абзаца)
            lead = self._safe_extract(
                soup,
                [
                    '.news-head__lead',
                    '.article-lead',
                    '.lead-text'
                ],
                'лид'
            )
            news_data['lead'] = lead

            # 5. Парсинг основного содержимого
            content_elements = self._safe_select(
                soup,
                [
                    '.news-text p',
                    '.article-content p',
                    '.news-content p',
                    '.text-content p'
                ],
                'основной текст'
            )
            
            if content_elements:
                content_text = []
                for element in content_elements:
                    text = element.get_text(strip=True)
                    if text and len(text) > 10:  # Фильтруем слишком короткие элементы
                        content_text.append(text)
                
                news_data['content'] = '\n\n'.join(content_text)
            else:
                logger.warning("Не найден основной текст новости")

            # 6. Парсинг тегов
            tags_elements = self._safe_select(
                soup,
                [
                    '.news-tags__item',
                    '.tags a',
                    '.article-tags a'
                ],
                'теги'
            )
            
            if tags_elements:
                news_data['tags'] = [tag.get_text(strip=True) for tag in tags_elements if tag.get_text(strip=True)]

            # 7. Парсинг главного изображения
            image = self._safe_extract_attribute(
                soup,
                [
                    '.news-head__image img',
                    '.article-image img',
                    '.main-image img',
                    '.news-head picture img'
                ],
                'src',
                'главное изображение'
            )
            if image:
                news_data['image_url'] = urljoin(url, image)

            # Валидация результата
            if not news_data['title'] and not news_data['content']:
                logger.error("Новость не содержит ни заголовка, ни контента. Сохраняем HTML для отладки.")
                with open(debug_filename, 'w', encoding='utf-8') as f:
                    f.write(html)
                return None

            # Проверяем качество данных
            validation_result = self._validate_news_data(news_data)
            if not validation_result['is_valid']:
                logger.warning(f"Проблемы с качеством данных: {validation_result['warnings']}")

            news_data['success'] = True
            logger.info(f"Успешно распарсена новость: {news_data['title']}")
            return news_data

        except Exception as e:
            logger.exception(f"Критическая ошибка при парсинге страницы {url}")
            # Сохраняем HTML для последующего анализа
            with open(debug_filename, 'w', encoding='utf-8') as f:
                f.write(html)
            return None

    def _safe_extract(self, soup, selectors, field_name):
        """Безопасно извлекает текст по селекторам."""
        for selector in selectors:
            try:
                element = soup.select_one(selector)
                if element:
                    text = element.get_text(strip=True)
                    if text:
                        logger.debug(f"Найден {field_name} с селектором '{selector}': {text[:50]}...")
                        return text
            except Exception as e:
                logger.debug(f"Ошибка с селектором '{selector}' для {field_name}: {e}")
                continue
                
        logger.warning(f"Не удалось извлечь {field_name}. Испробованы селекторы: {selectors}")
        return None

    def _safe_select(self, soup, selectors, field_name):
        """Безопасно извлекает список элементов по селекторам."""
        for selector in selectors:
            try:
                elements = soup.select(selector)
                if elements:
                    logger.debug(f"Найдены элементы {field_name} с селектором '{selector}': {len(elements)} шт.")
                    return elements
            except Exception as e:
                logger.debug(f"Ошибка с селектором '{selector}' для {field_name}: {e}")
                continue
                
        logger.warning(f"Не удалось найти элементы {field_name}. Испробованы селекторы: {selectors}")
        return None

    def _safe_extract_attribute(self, soup, selectors, attribute, field_name):
        """Безопасно извлекает атрибут элемента по селекторам."""
        for selector in selectors:
            try:
                element = soup.select_one(selector)
                if element and element.get(attribute):
                    attr_value = element.get(attribute)
                    logger.debug(f"Найден {field_name} с селектором '{selector}': {attr_value}")
                    return attr_value
            except Exception as e:
                logger.debug(f"Ошибка с селектором '{selector}' для {field_name}: {e}")
                continue
                
        logger.warning(f"Не удалось извлечь {field_name}. Испробованы селекторы: {selectors}")
        return None

    def _parse_moslenta_date(self, date_str):
        """Парсит дату в специфичном для Moslenta формате."""
        if not date_str:
            return None
            
        try:
            # Убираем лишние слова и приводим к стандартному формату
            cleaned_date = re.sub(r'[^\d\s\.\:\-]', '', date_str.strip())
            
            # Пробуем разные форматы дат
            date_formats = [
                '%d.%m.%Y',
                '%Y-%m-%d',
                '%d %B %Y',
                '%B %d, %Y',
                '%H:%M, %d.%m.%Y'  # Формат времени и даты
            ]
            
            for fmt in date_formats:
                try:
                    parsed_date = datetime.strptime(cleaned_date, fmt)
                    return parsed_date.isoformat()
                except ValueError:
                    continue
                    
            # Если стандартные форматы не подошли, возвращаем оригинал
            logger.warning(f"Не удалось распарсить дату: {date_str}")
            return date_str
            
        except Exception as e:
            logger.warning(f"Ошибка при парсинге даты '{date_str}': {e}")
            return date_str

    def _validate_news_data(self, news_data):
        """Проверяет качество распарсенных данных."""
        warnings = []
        
        if not news_data['title']:
            warnings.append("Отсутствует заголовок")
        elif len(news_data['title']) < 5:
            warnings.append("Слишком короткий заголовок")
            
        if not news_data['content']:
            warnings.append("Отсутствует основной текст")
        elif len(news_data['content']) < 100:
            warnings.append("Слишком короткий основной текст")
            
        if not news_data['date']:
            warnings.append("Отсутствует дата публикации")
            
        return {
            'is_valid': len(warnings) == 0,
            'warnings': warnings
        }

    def close(self):
        """Закрывает сессию."""
        self.session.close()

def main():
    """Основная функция для тестирования парсера."""
    url = "https://moslenta.ru/news/lyudi/dvukhletnii-rebenok-vypal-iz-okna-v-podmoskove-i-upal-v-korzinu-dlya-kondicionera-06-10-2025.htm"
    
    parser = MoslentaNewsParser()
    
    try:
        news = parser.parse_news_page(url)
        
        if news and news['success']:
            print("=" * 80)
            print("НОВОСТЬ УСПЕШНО РАСПАРСЕНА")
            print("=" * 80)
            print(f"Заголовок: {news['title']}")
            print(f"Дата: {news['date']}")
            print(f"Автор: {news['author']}")
            print(f"Лид: {news['lead']}")
            print(f"Теги: {', '.join(news['tags']) if news['tags'] else 'Нет тегов'}")
            print(f"Изображение: {news['image_url']}")
            print(f"URL: {news['url']}")
            print(f"Время парсинга: {news['parsing_time']}")
            print("\nСОДЕРЖИМОЕ:")
            print("-" * 40)
            print(news['content'][:500] + "..." if news['content'] and len(news['content']) > 500 else news['content'])
        else:
            print("Не удалось распарсить новость. Проверьте логи в файле moslenta_parser.log")
            
    except KeyboardInterrupt:
        logger.info("Парсинг прерван пользователем")
    except Exception as e:
        logger.error(f"Неожиданная ошибка в main: {e}")
    finally:
        parser.close()

if __name__ == "__main__":
    main()
