# -*- coding: utf-8 -*-
"""parser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vL-Ig4vu5GwLfbiNVnEIBN6isY3rZffm
"""

import requests
from bs4 import BeautifulSoup
from time import sleep
import csv
from urllib.parse import urljoin
import logging
import sys
from requests.exceptions import RequestException, Timeout, ConnectionError, HTTPError
from bs4 import MarkupResemblesLocatorWarning
import warnings
import os

# Настройка логирования
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('parser.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Игнорируем предупреждения BeautifulSoup
warnings.filterwarnings("ignore", category=MarkupResemblesLocatorWarning)

class ParserException(Exception):
    """Кастомное исключение для парсера"""
    pass

def setup_headers():
    """Настройка заголовков для запросов"""
    return {
        "User-Agent": (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/117.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "ru,en;q=0.9",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Referer": "https://lenta.ru/",
        "DNT": "1"
    }

def make_request(url, max_retries=3, timeout=10):
    """
    Выполняет HTTP запрос с обработкой ошибок и повторными попытками
    
    Args:
        url (str): URL для запроса
        max_retries (int): Максимальное количество попыток
        timeout (int): Таймаут запроса в секундах
    
    Returns:
        Response object или None в случае ошибки
    """
    headers = setup_headers()
    
    for attempt in range(max_retries):
        try:
            logger.info(f"Запрос к {url} (попытка {attempt + 1}/{max_retries})")
            response = requests.get(
                url, 
                headers=headers, 
                timeout=timeout,
                allow_redirects=True
            )
            response.raise_for_status()
            
            # Проверка на валидный HTML
            if not response.text.strip():
                logger.warning(f"Пустой ответ от {url}")
                continue
                
            return response
            
        except Timeout:
            logger.warning(f"Таймаут при запросе к {url} (попытка {attempt + 1})")
            if attempt == max_retries - 1:
                logger.error(f"Превышено максимальное количество попыток для {url}")
                return None
            sleep(2 ** attempt)  # Экспоненциальная задержка
            
        except ConnectionError as e:
            logger.warning(f"Ошибка подключения к {url}: {e} (попытка {attempt + 1})")
            if attempt == max_retries - 1:
                logger.error(f"Не удалось подключиться к {url}")
                return None
            sleep(2 ** attempt)
            
        except HTTPError as e:
            logger.error(f"HTTP ошибка {response.status_code} для {url}: {e}")
            if response.status_code in [403, 404]:
                return None  # Бесполезно повторять для этих кодов
            sleep(2 ** attempt)
            
        except RequestException as e:
            logger.error(f"Ошибка запроса к {url}: {e}")
            if attempt == max_retries - 1:
                return None
            sleep(2 ** attempt)
            
        except Exception as e:
            logger.error(f"Неожиданная ошибка при запросе к {url}: {e}")
            return None
    
    return None

def safe_soup_parse(html_text, parser='lxml'):
    """
    Безопасный парсинг HTML с обработкой ошибок
    """
    try:
        return BeautifulSoup(html_text, parser)
    except Exception as e:
        logger.error(f"Ошибка парсинга HTML: {e}")
        return None

def get_url(parser_dict: dict, url: str, max_pages=100, current_page=0):
    """
    Рекурсивный сбор URL новостей с обработкой корнер-кейсов
    
    Args:
        parser_dict (dict): Словарь для накопления результатов
        url (str): URL для парсинга
        max_pages (int): Максимальное количество страниц для обхода
        current_page (int): Текущая страница (для ограничения рекурсии)
    
    Returns:
        dict: Словарь с названиями и URL новостей
    """
    # Защита от бесконечной рекурсии
    if current_page >= max_pages:
        logger.warning(f"Достигнут лимит в {max_pages} страниц")
        return parser_dict
    
    # Проверка валидности URL
    if not url or not isinstance(url, str):
        logger.error("Невалидный URL")
        return parser_dict
    
    response = make_request(url)
    if not response:
        return parser_dict
        
    soup = safe_soup_parse(response.text)
    if not soup:
        return parser_dict
    
    # Поиск новостных карточек с обработкой различных случаев
    try:
        general_data = soup.find_all('a', class_='card-full-news _parts-news')
        
        if not general_data:
            logger.warning(f"Не найдено новостей на странице {url}")
            # Попробуем альтернативные селекторы
            alternative_selectors = [
                'a[class*="card-full-news"]',
                'a[class*="news"]',
                '.news-item a',
                '.item a'
            ]
            
            for selector in alternative_selectors:
                general_data = soup.select(selector)
                if general_data:
                    logger.info(f"Найдены новости с альтернативным селектором: {selector}")
                    break
        
        news_count = 0
        for data in general_data:
            try:
                # Безопасное извлечение заголовка
                title_elem = data.find('h3', class_='card-full-news__title')
                if not title_elem:
                    # Попробуем альтернативные селекторы для заголовка
                    title_elem = (data.find('h3') or 
                                 data.find('h2') or 
                                 data.find('h1') or
                                 data.find(attrs={'class': lambda x: x and 'title' in str(x)}))
                
                if title_elem and title_elem.text.strip():
                    name = title_elem.text.strip()
                else:
                    name = "Без названия"
                
                # Безопасное извлечение ссылки
                href = data.get("href")
                if not href:
                    continue
                    
                full_url = urljoin("https://lenta.ru", href)
                
                # Проверка на дубликаты
                if name in parser_dict:
                    logger.debug(f"Дубликат новости: {name}")
                    continue
                
                parser_dict[name] = full_url
                news_count += 1
                
                # Корректная задержка между обработкой элементов
                sleep(0.5)
                
            except Exception as e:
                logger.error(f"Ошибка при обработке элемента новости: {e}")
                continue
        
        logger.info(f"Страница {current_page + 1}: найдено {news_count} новостей")
        
        # Поиск кнопки "Загрузить еще" с обработкой различных случаев
        add_data = soup.find('a', class_='loadmore js-loadmore')
        if not add_data:
            # Попробуем альтернативные селекторы для кнопки
            alternative_loadmore = [
                'a[class*="loadmore"]',
                'a[class*="more"]',
                '.load-more a',
                '.more-news a',
                'button[class*="loadmore"]'
            ]
            
            for selector in alternative_loadmore:
                add_data = soup.select_one(selector)
                if add_data:
                    logger.info(f"Найдена кнопка загрузки с селектором: {selector}")
                    break
        
        if add_data and add_data.get('href'):
            next_url = urljoin('https://lenta.ru', add_data.get('href'))
            
            # Проверка на циклические ссылки
            if next_url == url or next_url in [v for v in parser_dict.values()]:
                logger.warning("Обнаружена циклическая ссылка, останавливаем рекурсию")
                return parser_dict
                
            logger.info(f"Переход на следующую страницу: {next_url}")
            return get_url(parser_dict, next_url, max_pages, current_page + 1)
        else:
            logger.info("Кнопка 'Загрузить еще' не найдена, завершаем сбор URL")
            return parser_dict
            
    except Exception as e:
        logger.error(f"Критическая ошибка при парсинге страницы {url}: {e}")
        return parser_dict

def initialize_csv():
    """Инициализация CSV файла с проверкой существования"""
    filename = "parser.csv"
    file_exists = os.path.isfile(filename)
    
    # Если файл существует, создаем резервную копию
    if file_exists:
        backup_name = f"parser_backup_{os.path.getctime(filename)}.csv"
        try:
            os.rename(filename, backup_name)
            logger.info(f"Создана резервная копия: {backup_name}")
        except Exception as e:
            logger.error(f"Не удалось создать резервную копию: {e}")
    
    # Создаем новый файл с заголовками
    try:
        with open(filename, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=["title", "text", "url", "status"])
            writer.writeheader()
        logger.info(f"Инициализирован CSV файл: {filename}")
    except Exception as e:
        logger.error(f"Ошибка при создании CSV файла: {e}")
        raise ParserException(f"Не удалось создать CSV файл: {e}")

def get_text(parser_dict: dict, batch_size=50):
    """
    Парсинг текста новостей с обработкой ошибок и батчингом
    
    Args:
        parser_dict (dict): Словарь с названиями и URL новостей
        batch_size (int): Размер батча для периодического сохранения
    """
    if not parser_dict:
        logger.error("Словарь новостей пуст")
        return
    
    total_news = len(parser_dict)
    logger.info(f"Начало парсинга текста для {total_news} новостей")
    
    successful = 0
    failed = 0
    skipped = 0
    
    # Инициализация CSV
    try:
        initialize_csv()
    except ParserException as e:
        logger.error(f"Не удалось инициализировать CSV: {e}")
        return
    
    for i, (title, url) in enumerate(parser_dict.items(), 1):
        try:
            logger.info(f"Обработка {i}/{total_news}: {title[:50]}...")
            
            # Пропускаем невалидные URL
            if not url or not url.startswith('http'):
                logger.warning(f"Пропущен невалидный URL: {url}")
                skipped += 1
                status = "skipped_invalid_url"
                save_to_csv(title, "", url, status)
                continue
            
            response = make_request(url)
            if not response:
                failed += 1
                status = "failed_request"
                save_to_csv(title, "", url, status)
                continue
            
            soup = safe_soup_parse(response.text)
            if not soup:
                failed += 1
                status = "failed_parsing"
                save_to_csv(title, "", url, status)
                continue
            
            # Поиск контента с альтернативными селекторами
            general_data = soup.find('div', class_='topic-page__container')
            if not general_data:
                # Попробуем альтернативные селекторы для контейнера контента
                alternative_containers = [
                    '.article-content',
                    '.news-content',
                    '.content',
                    '.post-content',
                    '[class*="content"]',
                    '[class*="article"]'
                ]
                
                for selector in alternative_containers:
                    general_data = soup.select_one(selector)
                    if general_data:
                        logger.debug(f"Найден контент с селектором: {selector}")
                        break
            
            if not general_data:
                logger.warning(f"Не найден контент для {url}")
                failed += 1
                status = "no_content"
                save_to_csv(title, "", url, status)
                continue
            
            # Извлечение текста с различными селекторами
            text_parts = []
            text_selectors = [
                'p.topic-body__content-text',
                'p',
                'div[class*="content"] p',
                'article p',
                '.text p'
            ]
            
            for selector in text_selectors:
                paragraphs = general_data.select(selector)
                if paragraphs:
                    text_parts = [p.get_text().strip() for p in paragraphs if p.get_text().strip()]
                    if text_parts:
                        logger.debug(f"Текст извлечен с селектором: {selector}")
                        break
            
            if not text_parts:
                logger.warning(f"Не найден текст для {url}")
                failed += 1
                status = "no_text"
                save_to_csv(title, "", url, status)
                continue
            
            text = ' '.join(text_parts)
            
            # Проверка на минимальную длину текста
            if len(text) < 50:
                logger.warning(f"Слишком короткий текст ({len(text)} символов) для {url}")
                status = "short_text"
            else:
                status = "success"
                successful += 1
            
            save_to_csv(title, text, url, status)
            
            # Периодическое логирование прогресса
            if i % batch_size == 0:
                logger.info(f"Обработано {i}/{total_news} новостей")
            
            # Задержка между запросами
            sleep(1)
            
        except Exception as e:
            logger.error(f"Неожиданная ошибка при обработке {url}: {e}")
            failed += 1
            save_to_csv(title, "", url, "unexpected_error")
            continue
    
    # Финальная статистика
    logger.info(
        f"Завершено! Успешно: {successful}, Неудачно: {failed}, "
        f"Пропущено: {skipped}, Всего: {total_news}"
    )

def save_to_csv(title, text, url, status):
    """
    Безопасное сохранение в CSV с обработкой ошибок
    """
    try:
        with open("parser.csv", "a", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=["title", "text", "url", "status"])
            writer.writerow({
                "title": title[:500],  # Ограничение длины заголовка
                "text": text[:10000],  # Ограничение длины текста
                "url": url,
                "status": status
            })
    except Exception as e:
        logger.error(f"Ошибка при записи в CSV: {e}")

def main():
    """Основная функция с обработкой исключений верхнего уровня"""
    try:
        logger.info("Запуск парсера Lenta.ru")
        
        # Сбор URL
        url = 'https://lenta.ru/parts/news/'
        logger.info(f"Начало сбора URL с {url}")
        
        parser_dict = get_url(dict(), url)
        
        if not parser_dict:
            logger.error("Не удалось собрать URL новостей")
            return
        
        logger.info(f"Собрано {len(parser_dict)} URL новостей")
        
        # Парсинг текста
        logger.info("Начало парсинга текста новостей")
        get_text(parser_dict)
        
        logger.info("Парсинг завершен успешно")
        
    except KeyboardInterrupt:
        logger.info("Парсинг прерван пользователем")
    except Exception as e:
        logger.error(f"Критическая ошибка в main: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
